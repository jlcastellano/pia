<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluación y Optimización de Modelos de Machine Learning</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div>Evaluación y Optimización de Modelos de Machine Learning</div>
    </header>

    <section class="contenido-didactico">
        <h1>¿Cómo sabemos si nuestro modelo de machine learning funciona bien?</h1>
        
        <p>Cuando entrenamos un modelo de inteligencia artificial, es como enseñarle a un estudiante. Después del entrenamiento, necesitamos evaluar qué tan bien ha aprendido. Para esto usamos la función <strong>evaluate</strong>, que nos da dos valores importantes:</p>
        
        <ul>
            <li>El <strong>Loss</strong> (error): qué tan equivocado está el modelo</li>
            <li>El <strong>Accuracy</strong> (precisión o tasa de acierto): qué porcentaje de respuestas son correctas</li>
        </ul>
        
        <p>Es muy común que cuando probamos el modelo con datos nuevos, su rendimiento sea menor que durante el entrenamiento.</p>
        
        <figure>
            <img src="https://pfst.cf2.poecdn.net/base/image/5a8293d264ae8b08173213dfa5429d5fe574b15c476cc286cbfb688fc11c563c?w=1536&h=1024" alt="Diagrama mostrando la diferencia entre rendimiento en entrenamiento y prueba">
            <figcaption>Diferencia entre rendimiento en entrenamiento vs prueba.</figcaption>
        </figure>
        
        <p>Esto es como cuando un estudiante saca buenas notas en los ejercicios de práctica pero le va peor en el examen final. Significa que el modelo memorizó los datos de entrenamiento pero no aprendió a generalizar.</p>
        
        <p>Para realizar esta evaluación correctamente, necesitamos dividir nuestros datos etiquetados en dos grupos antes de empezar:</p>
        
        <ul>
            <li><strong>Datos de entrenamiento (train)</strong>: para que el modelo aprenda</li>
            <li><strong>Datos de prueba (test)</strong>: para evaluar qué tan bien aprendió</li>
        </ul>
        
        <p>En Keras (ahora integrado en TensorFlow 2.x en 2025), usamos la función <span class="inline-code">train_test_split</span> para hacer esta división. Por ejemplo:</p>
        
        <pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)</code></pre>
        
        <p>En este ejemplo, reservamos el 75% de los datos para entrenar y el 25% para probar. El parámetro <span class="inline-code">random_state</span> asegura que la división sea reproducible.</p>
        
        <p>Veamos cómo funciona en la práctica:</p>
        
        <h2>Paso 1: Entrenamiento del modelo</h2>
        
        <p>Utilizamos la función <span class="inline-code">fit</span> para entrenar nuestra red neuronal profunda (DNN). Necesitamos proporcionar:</p>
        
        <ul>
            <li>Los datos de entrada (train_images)</li>
            <li>Las etiquetas o respuestas correctas (train_labels)</li>
            <li>El número de iteraciones completas sobre los datos (epochs)</li>
        </ul>
        
        <pre><code class="language-python">model.fit(train_images, train_labels, epochs=5)</code></pre>
        
        <h2>Paso 2: Evaluación del modelo</h2>
        
        <p>Después del entrenamiento, usamos la función <span class="inline-code">evaluate</span> con los datos que el modelo nunca ha visto:</p>
        
        <pre><code class="language-python">test_loss, test_acc = model.evaluate(test_images, test_labels)
print('Precisión en el test:', test_acc)</code></pre>
        
        <p>En nuestro ejemplo, el modelo alcanzó una precisión del 87% con datos nuevos, comparado con el 89% que había logrado durante el entrenamiento. Esta pequeña diferencia es normal y aceptable.</p>
        
        <h2>El problema del sobreentrenamiento (overfitting)</h2>
        
        <p>A veces, nuestros datos contienen casos atípicos o "ruido": información incorrecta, inconsistente o poco representativa de la realidad. Durante el entrenamiento, el modelo intenta adaptarse también a estos datos anómalos, aprendiendo patrones que en realidad no son útiles. Es como memorizar las respuestas específicas de unos ejercicios en lugar de entender los conceptos.</p>
        
        <p>Este problema se llama <strong>overfitting</strong> o sobreentrenamiento. Lo difícil es que no lo detectamos hasta después de entrenar el modelo y probarlo con datos nuevos. Como algunos entrenamientos pueden durar horas o días y consumir muchos recursos, sería ideal poder monitorear el proceso mientras ocurre.</p>
        
        <h2>La solución: datos de validación</h2>
        
        <p>Para esto, creamos un tercer conjunto de datos llamado <strong>conjunto de validación</strong>. Separamos una porción adicional de los datos de entrenamiento y los usamos para evaluar el modelo al final de cada iteración (epoch). Así podemos ver cómo evoluciona el rendimiento con datos que no participan en el ajuste del modelo. Es como hacer mini-exámenes después de cada clase para ver si vamos por buen camino.</p>
        
        <h2>Ejemplo práctico: Clasificación de valoraciones de películas con deep learning</h2>
        
        <p>Vamos a trabajar con el dataset IMDB, que contiene reseñas de películas. Este conjunto de datos viene ya preprocesado en Keras/TensorFlow, donde el texto se ha convertido en secuencias de números enteros que representan palabras.</p>
        
        <h3>Cargando los datos:</h3>
        
        <pre><code class="language-python">from keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)</code></pre>
        
        <p>¿Qué contienen estos datos?</p>
        
        <ul>
            <li><span class="inline-code">train_data</span> y <span class="inline-code">test_data</span>: listas de números que representan las palabras de cada reseña</li>
            <li><span class="inline-code">train_labels</span> y <span class="inline-code">test_labels</span>: listas de 0 y 1, donde 0 significa valoración negativa y 1 significa valoración positiva</li>
        </ul>
        
        <p>El dataset incluye la función <span class="inline-code">get_word_index</span> que nos permite ver cómo se codificó el texto. Cada palabra recibe un número según su frecuencia de aparición: las palabras más comunes tienen números más bajos.</p>
        
        <h3>Preparando los datos de entrada:</h3>
        
        <p>Las redes neuronales solo pueden trabajar con estructuras de datos regulares llamadas tensores. No podemos introducir listas de diferentes longitudes. Por eso, necesitamos "vectorizar" nuestras listas de comentarios usando una técnica llamada <strong>one-hot encoding</strong>, que convierte cada lista en un vector de tamaño fijo.</p>
        
        <p>También convertimos las etiquetas en arrays numéricos:</p>
        
        <pre><code class="language-python">y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')</code></pre>
        
        <h3>Construyendo el modelo:</h3>
        
        <p>Creamos un modelo Sequential (secuencial) con 3 capas de neuronas:</p>
        
        <ul>
            <li>Dos capas ocultas con 16 neuronas cada una</li>
            <li>Una capa de salida con 1 neurona (porque es un problema de clasificación binaria: positivo o negativo)</li>
        </ul>
        
        <pre><code class="language-python">from keras import models, layers
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))</code></pre>
        
        <h3>Configurando el entrenamiento:</h3>
        
        <p>Definimos cómo queremos que aprenda el modelo:</p>
        
        <pre><code class="language-python">model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])</code></pre>
        
        <h3>Reservando datos para validación:</h3>
        
        <p>Separamos los primeros 10,000 ejemplos para validación:</p>
        
        <pre><code class="language-python">x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]</code></pre>
        
        <h3>Entrenando el modelo:</h3>
        
        <pre><code class="language-python">training = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))</code></pre>
        
        <h3>Analizando la evolución:</h3>
        
        <p>Cuando representamos gráficamente el error y la precisión en cada epoch, podemos detectar el overfitting:</p>
        
        <figure>
            <img src="https://pfst.cf2.poecdn.net/base/image/571f3f2241d903a1afdcd055c2e599c9753a5c1fc19621dbd55bd27854759ba6?w=1536&h=1024" alt="Gráfica mostrando cómo la precisión de validación empeora mientras la de entrenamiento mejora">
            <figcaption>El concepto de overfitting es crítico y altamente abstracto.</figcaption>
        </figure>
        
        <p>Si la precisión con datos de entrenamiento sigue subiendo pero la de validación se estanca o baja, estamos sobreentrenando. En nuestro ejemplo, esto ocurre alrededor del epoch 4 o 5.</p>
        
        <p>La solución es simple: entrenar un nuevo modelo pero solo con 5 epochs, antes de que empiece el overfitting.</p>
        
        <h3>Para profundizar más:</h3>
        
        <p>Si quieres aprender más sobre estos temas, puedes consultar:</p>
        
        <ul>
            <li>La documentación del método <span class="inline-code">evaluate</span> en TensorFlow/Keras (en 2025, Keras está completamente integrado en TensorFlow 2.x)</li>
            <li>La guía oficial sobre entrenamiento y evaluación en la documentación de TensorFlow</li>
        </ul>
    </section>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="script.js"></script>
</body>
</html>