<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluación y Optimización de Modelos de Machine Learning</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div>Evaluación y Optimización de Modelos de Machine Learning</div>
    </header>

    <section class="contenido-didactico">
        <h1>Decaimiento de los pesos (Weight Decay)</h1>
        
        <p>Dentro de las técnicas de Regularización, el decaimiento de los pesos es bastante intuitivo y fácil de implementar. El objetivo es simplificar el modelo limitando los valores que pueden tomar los pesos. Si mantenemos los pesos en valores bajos, su distribución es más regular, menos caótica, y el modelo está más controlado.</p>
        
        <h2>¿Cómo funciona?</h2>
        
        <p>Durante el entrenamiento, el modelo intenta minimizar el error. Con esta técnica, añadimos una penalización adicional: si los pesos crecen mucho, aumentamos artificialmente el error. Así, el modelo aprende a mantener los pesos pequeños.</p>
        
        <p>Existen dos formas principales de aplicar esta penalización:</p>
        
        <ul>
            <li><strong>Regularización L1:</strong> la penalización es proporcional al valor absoluto de cada peso. Tiende a crear modelos más "dispersos" (muchos pesos exactamente en cero).</li>
            <li><strong>Regularización L2</strong> (también llamada Weight Decay o decaimiento de los pesos): la penalización es proporcional al cuadrado del valor de cada peso. Es la más utilizada y tiende a mantener todos los pesos pequeños pero no exactamente en cero.</li>
        </ul>
        
        <h2>Implementación en Keras/TensorFlow:</h2>
        
        <p>Para aplicar regularización en Keras, lo indicamos al crear cada capa, usando el parámetro <span class="inline-code">kernel_regularizer</span>:</p>
        
        <pre><code class="language-python">from keras import regularizers
model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,)))</code></pre>
        
        <p>En este ejemplo, el coeficiente 0.001 controla cuánta penalización aplicamos. Por cada peso de la red, se añade al error total el producto de 0.001 por el cuadrado del peso.</p>
        
        <p class="nota"><strong>Consecuencia importante:</strong> cuando usamos regularización, el error durante el entrenamiento será visiblemente mayor que el error de validación o test. Esto es normal y esperado, porque estamos añadiendo una penalización artificial durante el entrenamiento que no existe en la evaluación.</p>
    </section>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="script.js"></script>
</body>
</html>