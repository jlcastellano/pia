{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de Hiperparámetros en Deep Learning\n",
    "\n",
    "Este notebook presenta ejemplos prácticos de las principales herramientas para optimización de hiperparámetros mencionadas en el documento de evaluación y optimización de modelos de Machine Learning.\n",
    "\n",
    "## ¿Qué son los hiperparámetros?\n",
    "\n",
    "Los hiperparámetros son valores que **nosotros decidimos antes del entrenamiento** y que no son aprendidos por el modelo. Ejemplos incluyen:\n",
    "\n",
    "- Número de capas\n",
    "- Número de neuronas por capa\n",
    "- Tasa de aprendizaje (learning rate)\n",
    "- Tasa de dropout\n",
    "- Tipo de optimizador\n",
    "\n",
    "Encontrar la combinación óptima de estos valores es crucial para obtener el mejor rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Keras Tuner\n",
    "\n",
    "### ¿Qué es?\n",
    "**Keras Tuner** es la solución oficial de TensorFlow/Keras para la búsqueda automatizada de hiperparámetros. Fue desarrollada por el equipo de Keras y se integra perfectamente con el ecosistema de TensorFlow.\n",
    "\n",
    "### Características principales:\n",
    "- **Integración nativa**: Diseñada específicamente para modelos Keras/TensorFlow\n",
    "- **Múltiples algoritmos**: Soporta búsqueda aleatoria, bayesiana e Hyperband\n",
    "- **Fácil de usar**: API intuitiva y bien documentada\n",
    "- **Reanudable**: Puede continuar búsquedas interrumpidas\n",
    "\n",
    "### Algoritmos disponibles:\n",
    "- `RandomSearch`: Prueba combinaciones aleatorias\n",
    "- `BayesianOptimization`: Usa modelos probabilísticos para guiar la búsqueda\n",
    "- `Hyperband`: Asigna recursos adaptativamente, descartando configuraciones pobres\n",
    "\n",
    "### Cuándo usarlo:\n",
    "- Cuando trabajas exclusivamente con TensorFlow/Keras\n",
    "- Para proyectos que requieren una solución simple y bien integrada\n",
    "- Cuando necesitas soporte oficial y documentación extensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Instalación de Keras Tuner\n",
    "!pip install keras-tuner -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "Keras Tuner version: 1.4.8\n"
     ]
    }
   ],
   "source": [
    "# Importaciones necesarias\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras Tuner version: {kt.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo con hiperparámetros buscables\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Construye un modelo con hiperparámetros definidos por el tuner.\n",
    "    \n",
    "    hp.Int(): Define un hiperparámetro entero\n",
    "    hp.Float(): Define un hiperparámetro de punto flotante\n",
    "    hp.Choice(): Define un hiperparámetro categórico\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    \n",
    "    # Buscar el número óptimo de capas (entre 1 y 3)\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        # Buscar el número óptimo de neuronas (32 a 512, en pasos de 32)\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        # Buscar el dropout óptimo (0.0 a 0.5)\n",
    "        model.add(layers.Dropout(hp.Float(f'dropout_{i}', 0.0, 0.5, step=0.1)))\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Buscar la tasa de aprendizaje óptima (escala logarítmica)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "        ),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Datos de entrenamiento: (60000, 28, 28)\n",
      "Datos de prueba: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Cargar y preparar datos MNIST\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(f\"Datos de entrenamiento: {x_train.shape}\")\n",
    "print(f\"Datos de prueba: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "dropout_0 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\github\\pia\\venv310\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Configurar el tuner con búsqueda bayesiana\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',      # Métrica a optimizar\n",
    "    max_trials=20,                  # Número máximo de configuraciones a probar\n",
    "    directory='keras_tuner_results',\n",
    "    project_name='mnist_optimization'\n",
    ")\n",
    "\n",
    "# Ver resumen del espacio de búsqueda\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [00h 01m 28s]\n",
      "val_accuracy: 0.9745833277702332\n",
      "\n",
      "Best val_accuracy So Far: 0.9794166684150696\n",
      "Total elapsed time: 00h 11m 58s\n",
      "\n",
      "Search: Running Trial #7\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1                 |3                 |num_layers\n",
      "320               |320               |units_0\n",
      "0.3               |0.2               |dropout_0\n",
      "0.00013759        |0.00020635        |learning_rate\n",
      "224               |480               |units_1\n",
      "0                 |0.3               |dropout_1\n",
      "352               |32                |units_2\n",
      "0.3               |0                 |dropout_2\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.8456 - loss: 0.5662 - val_accuracy: 0.9270 - val_loss: 0.2699\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.9223 - loss: 0.2735 - val_accuracy: 0.9457 - val_loss: 0.2056\n",
      "Epoch 3/10\n",
      "\u001b[1m 431/1500\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9392 - loss: 0.2174"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ejecutar la búsqueda de hiperparámetros\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    242\u001b[0m     ):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m         )\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    239\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    240\u001b[0m     ):\n\u001b[1;32m--> 241\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32me:\\github\\pia\\venv310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ejecutar la búsqueda de hiperparámetros\n",
    "tuner.search(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los mejores hiperparámetros\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MEJORES HIPERPARÁMETROS ENCONTRADOS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Número de capas: {best_hps.get('num_layers')}\")\n",
    "print(f\"Learning rate: {best_hps.get('learning_rate'):.6f}\")\n",
    "\n",
    "for i in range(best_hps.get('num_layers')):\n",
    "    print(f\"Capa {i+1} - Unidades: {best_hps.get(f'units_{i}')}, Dropout: {best_hps.get(f'dropout_{i}'):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo final con los mejores hiperparámetros\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = best_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluar en datos de prueba\n",
    "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
    "print(f\"\\nPrecisión en test: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Optuna\n",
    "\n",
    "### ¿Qué es?\n",
    "**Optuna** es un framework moderno y eficiente para optimización de hiperparámetros. Es agnóstico al framework de deep learning, lo que significa que funciona con TensorFlow, PyTorch, scikit-learn y cualquier otra biblioteca.\n",
    "\n",
    "### Características principales:\n",
    "- **Poda inteligente (Pruning)**: Detiene automáticamente entrenamientos que no son prometedores\n",
    "- **Visualizaciones integradas**: Gráficos interactivos para analizar resultados\n",
    "- **Define-by-run**: API flexible que permite definir el espacio de búsqueda dinámicamente\n",
    "- **Paralelización**: Soporta búsquedas distribuidas\n",
    "- **Persistencia**: Guarda estudios en bases de datos (SQLite, MySQL, PostgreSQL)\n",
    "\n",
    "### Conceptos clave:\n",
    "- **Study**: Un proceso de optimización completo\n",
    "- **Trial**: Una ejecución individual con una configuración específica\n",
    "- **Objective**: La función que queremos minimizar o maximizar\n",
    "\n",
    "### Cuándo usarlo:\n",
    "- Cuando necesitas flexibilidad para usar diferentes frameworks\n",
    "- Para proyectos que requieren poda inteligente de trials\n",
    "- Cuando quieres visualizaciones detalladas del proceso de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de Optuna\n",
    "!pip install optuna plotly -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_slice\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Silenciar logs de TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(f\"Optuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y preparar datos\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Separar datos de validación\n",
    "x_val, y_val = x_train[:10000], y_train[:10000]\n",
    "x_train, y_train = x_train[10000:], y_train[10000:]\n",
    "\n",
    "print(f\"Entrenamiento: {x_train.shape}\")\n",
    "print(f\"Validación: {x_val.shape}\")\n",
    "print(f\"Test: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función objetivo\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Función objetivo que Optuna intentará maximizar.\n",
    "    \n",
    "    trial.suggest_int(): Sugiere un valor entero\n",
    "    trial.suggest_float(): Sugiere un valor flotante\n",
    "    trial.suggest_categorical(): Sugiere un valor de una lista\n",
    "    \"\"\"\n",
    "    # Sugerir hiperparámetros\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    \n",
    "    # Construir modelo\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        n_units = trial.suggest_int(f'n_units_l{i}', 32, 256)\n",
    "        dropout_rate = trial.suggest_float(f'dropout_l{i}', 0.1, 0.5)\n",
    "        model.add(layers.Dense(n_units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Configurar optimizador\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callback para poda (pruning)\n",
    "    class OptunaCallback(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            # Reportar valor intermedio\n",
    "            trial.report(logs['val_accuracy'], epoch)\n",
    "            # Verificar si debemos podar este trial\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "    \n",
    "    # Entrenar\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=[OptunaCallback()],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y ejecutar el estudio\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',  # Queremos maximizar la accuracy\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5,   # Trials antes de empezar a podar\n",
    "        n_warmup_steps=5      # Epochs antes de podar un trial\n",
    "    ),\n",
    "    study_name='mnist_optimization'\n",
    ")\n",
    "\n",
    "# Ejecutar optimización\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=30,\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver resultados\n",
    "print(\"=\" * 50)\n",
    "print(\"RESULTADOS DE OPTUNA\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Número de trials completados: {len(study.trials)}\")\n",
    "print(f\"Mejor valor de accuracy: {study.best_trial.value:.4f}\")\n",
    "print(f\"\\nMejores hiperparámetros:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización: Historia de optimización\n",
    "fig = plot_optimization_history(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización: Importancia de parámetros\n",
    "fig = plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización: Coordenadas paralelas\n",
    "fig = plot_parallel_coordinate(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Ray Tune\n",
    "\n",
    "### ¿Qué es?\n",
    "**Ray Tune** es una biblioteca para búsqueda de hiperparámetros distribuida y escalable. Es parte del ecosistema Ray, diseñado para computación distribuida de alto rendimiento.\n",
    "\n",
    "### Características principales:\n",
    "- **Escalabilidad**: Ejecuta búsquedas en clusters de múltiples máquinas\n",
    "- **Schedulers avanzados**: ASHA, PBT (Population Based Training), etc.\n",
    "- **Integración**: Compatible con Optuna, HyperOpt, y otros algoritmos\n",
    "- **Tolerancia a fallos**: Recuperación automática de experimentos fallidos\n",
    "- **Soporte multi-GPU**: Distribución eficiente de recursos\n",
    "\n",
    "### Schedulers disponibles:\n",
    "- **ASHAScheduler**: Detención temprana agresiva pero eficiente\n",
    "- **PopulationBasedTraining**: Evoluciona hiperparámetros durante el entrenamiento\n",
    "- **MedianStoppingRule**: Detiene trials por debajo de la mediana\n",
    "\n",
    "### Cuándo usarlo:\n",
    "- Para búsquedas a gran escala con múltiples GPUs o máquinas\n",
    "- Cuando necesitas tolerancia a fallos en experimentos largos\n",
    "- Para proyectos de producción que requieren escalabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de Ray Tune\n",
    "!pip install \"ray[tune]\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Ray version: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Ray\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar datos\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento para Ray Tune\n",
    "def train_mnist(config):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento que recibe configuración de Ray Tune.\n",
    "    \n",
    "    config: diccionario con los hiperparámetros a probar\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = load_data()\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(config[\"hidden_units_1\"], activation='relu'),\n",
    "        layers.Dropout(config[\"dropout_1\"]),\n",
    "        layers.Dense(config[\"hidden_units_2\"], activation='relu'),\n",
    "        layers.Dropout(config[\"dropout_2\"]),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=config[\"learning_rate\"]),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callback para reportar métricas a Ray Tune\n",
    "    class TuneReportCallback(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            tune.report(\n",
    "                mean_accuracy=logs['val_accuracy'],\n",
    "                mean_loss=logs['val_loss']\n",
    "            )\n",
    "    \n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=20,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        callbacks=[TuneReportCallback()],\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el espacio de búsqueda\n",
    "search_space = {\n",
    "    \"hidden_units_1\": tune.choice([64, 128, 256, 512]),\n",
    "    \"hidden_units_2\": tune.choice([32, 64, 128, 256]),\n",
    "    \"dropout_1\": tune.uniform(0.1, 0.5),\n",
    "    \"dropout_2\": tune.uniform(0.1, 0.5),\n",
    "    \"learning_rate\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"batch_size\": tune.choice([32, 64, 128])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar scheduler ASHA para detención temprana\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",\n",
    "    max_t=20,           # Máximo de epochs\n",
    "    grace_period=5,     # Epochs mínimos antes de podar\n",
    "    reduction_factor=2  # Factor de reducción\n",
    ")\n",
    "\n",
    "# Usar Optuna como algoritmo de búsqueda\n",
    "search_alg = OptunaSearch(\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la búsqueda\n",
    "analysis = tune.run(\n",
    "    train_mnist,\n",
    "    config=search_space,\n",
    "    num_samples=20,           # Número de configuraciones a probar\n",
    "    scheduler=scheduler,\n",
    "    search_alg=search_alg,\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 0},  # Recursos por trial\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener mejores resultados\n",
    "best_config = analysis.get_best_config(metric=\"mean_accuracy\", mode=\"max\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"RESULTADOS DE RAY TUNE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Mejor configuración encontrada:\")\n",
    "for key, value in best_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver DataFrame con todos los resultados\n",
    "df = analysis.results_df\n",
    "print(\"\\nTop 5 configuraciones:\")\n",
    "df.sort_values(\"mean_accuracy\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apagar Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. TensorBoard HParams\n",
    "\n",
    "### ¿Qué es?\n",
    "**TensorBoard HParams** es un plugin de TensorBoard que permite visualizar y comparar de forma interactiva los resultados de múltiples experimentos con diferentes hiperparámetros.\n",
    "\n",
    "### Características principales:\n",
    "- **Visualización interactiva**: Tablas, gráficos de coordenadas paralelas y scatter plots\n",
    "- **Filtrado dinámico**: Filtra experimentos por rangos de hiperparámetros\n",
    "- **Comparación lado a lado**: Compara curvas de entrenamiento de diferentes configuraciones\n",
    "- **Integración con TensorBoard**: Usa la misma interfaz que ya conoces\n",
    "\n",
    "### Componentes clave:\n",
    "- **HParam**: Define un hiperparámetro y su dominio\n",
    "- **Metric**: Define las métricas a registrar\n",
    "- **hparams_config**: Configura el experimento\n",
    "\n",
    "### Cuándo usarlo:\n",
    "- Para visualizar y analizar resultados de búsquedas manuales o automatizadas\n",
    "- Cuando necesitas una interfaz visual para explorar el espacio de hiperparámetros\n",
    "- Para presentar resultados de experimentación de forma clara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import datetime\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir hiperparámetros a explorar\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([64, 128, 256]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.1, 0.3, 0.5]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-2, 1e-3, 1e-4]))\n",
    "\n",
    "# Métricas a registrar\n",
    "METRIC_ACCURACY = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el directorio de logs\n",
    "log_dir = \"logs/hparam_tuning/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Limpiar directorio si existe\n",
    "if os.path.exists(\"logs/hparam_tuning\"):\n",
    "    shutil.rmtree(\"logs/hparam_tuning\")\n",
    "\n",
    "# Configurar HParams en TensorBoard\n",
    "with tf.summary.create_file_writer(log_dir).as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER, HP_LEARNING_RATE],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento\n",
    "def train_test_model(hparams, run_dir):\n",
    "    \"\"\"Entrena un modelo con los hiperparámetros dados y retorna la accuracy.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hparams[HP_NUM_UNITS], activation='relu'),\n",
    "        layers.Dropout(hparams[HP_DROPOUT]),\n",
    "        layers.Dense(10, activation='softmax'),\n",
    "    ])\n",
    "    \n",
    "    # Configurar optimizador\n",
    "    if hparams[HP_OPTIMIZER] == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "    else:\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=hparams[HP_LEARNING_RATE], momentum=0.9)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    \n",
    "    # Entrenar con callback de TensorBoard\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=5,  # Reducido para demostración\n",
    "        validation_split=0.2,\n",
    "        callbacks=[keras.callbacks.TensorBoard(run_dir, histogram_freq=1)],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    _, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar experimentos con todas las combinaciones\n",
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "    for dropout_rate in HP_DROPOUT.domain.values:\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            for lr in HP_LEARNING_RATE.domain.values:\n",
    "                hparams = {\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_DROPOUT: dropout_rate,\n",
    "                    HP_OPTIMIZER: optimizer,\n",
    "                    HP_LEARNING_RATE: lr,\n",
    "                }\n",
    "                \n",
    "                run_name = f\"run-{session_num}\"\n",
    "                run_dir = log_dir + \"/\" + run_name\n",
    "                \n",
    "                print(f'--- Trial {session_num + 1}: units={num_units}, dropout={dropout_rate}, '\n",
    "                      f'opt={optimizer}, lr={lr}')\n",
    "                \n",
    "                # Entrenar y obtener accuracy\n",
    "                accuracy = train_test_model(hparams, run_dir)\n",
    "                print(f'    Accuracy: {accuracy:.4f}')\n",
    "                \n",
    "                # Registrar hiperparámetros y métrica en TensorBoard\n",
    "                with tf.summary.create_file_writer(run_dir).as_default():\n",
    "                    hp.hparams(hparams)\n",
    "                    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "                \n",
    "                session_num += 1\n",
    "\n",
    "print(f\"\\nTotal de experimentos: {session_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar extensión de TensorBoard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar en TensorBoard\n",
    "# Navega a la pestaña \"HPARAMS\" para ver las visualizaciones interactivas\n",
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resumen Comparativo\n",
    "\n",
    "| Herramienta | Ventaja Principal | Mejor Caso de Uso | Curva de Aprendizaje |\n",
    "|-------------|-------------------|-------------------|----------------------|\n",
    "| **Keras Tuner** | Integración nativa con Keras | Proyectos TensorFlow/Keras | Baja |\n",
    "| **Optuna** | Poda inteligente, visualizaciones | Cualquier framework, flexibilidad | Media |\n",
    "| **Ray Tune** | Escalabilidad distribuida | Clusters, múltiples GPUs | Alta |\n",
    "| **TensorBoard HParams** | Visualización interactiva | Análisis de resultados | Baja |\n",
    "\n",
    "### Recomendaciones:\n",
    "\n",
    "1. **Para principiantes**: Comienza con **Keras Tuner** si usas TensorFlow/Keras\n",
    "2. **Para proyectos medianos**: **Optuna** ofrece el mejor balance entre funcionalidad y facilidad\n",
    "3. **Para producción a escala**: **Ray Tune** cuando necesites distribuir la búsqueda\n",
    "4. **Para análisis visual**: Combina cualquier herramienta con **TensorBoard HParams**\n",
    "\n",
    "### Consejo del documento original:\n",
    "\n",
    "> *\"Aunque estas herramientas automáticas son útiles, es fundamental entender qué hace cada hiperparámetro para poder interpretar los resultados y tomar decisiones informadas sobre la arquitectura de tu modelo.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
