<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Programación de Redes Neuronales Profundas con Keras</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div>Programación de Redes Neuronales Profundas con Keras</div>
    </header>

    <section class="contenido-didactico">
        <h1>3. Funciones de Coste (Loss Functions)</h1>
        <p>La <strong>función de coste</strong> (también llamada función de pérdida o loss) cuantifica qué tan lejos están las predicciones del modelo de los valores reales. El entrenamiento consiste en encontrar los pesos que minimizan esta función.</p>

        <h2>3.1 Funciones para Clasificación</h2>

        <h3>Binary Crossentropy (Entropía Cruzada Binaria)</h3>
        <p class="formula">L = -1/N ∑<sub>i=1</sub><sup>N</sup>[y<sub>i</sub> log(ŷ<sub>i</sub>) + (1-y<sub>i</sub>)log(1-ŷ<sub>i</sub>)]</p>
        <ul>
            <li><strong>Uso</strong>: Clasificación binaria (2 clases)</li>
            <li><strong>Capa de salida</strong>: 1 neurona con activación sigmoid</li>
            <li><strong>Etiquetas</strong>: 0 o 1</li>
        </ul>

<pre><code class="language-python">model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)</code></pre>

        <h3>Categorical Crossentropy</h3>
        <p class="formula">L = -∑<sub>i=1</sub><sup>C</sup> y<sub>i</sub> log(ŷ<sub>i</sub>)</p>
        <ul>
            <li><strong>Uso</strong>: Clasificación multiclase</li>
            <li><strong>Capa de salida</strong>: N neuronas (una por clase) con activación softmax</li>
            <li><strong>Etiquetas</strong>: Formato one-hot encoding, ej: [0, 1, 0, 0]</li>
        </ul>

<pre><code class="language-python"># Etiquetas en formato one-hot
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)</code></pre>

        <h3>Sparse Categorical Crossentropy</h3>
        <ul>
            <li><strong>Uso</strong>: Clasificación multiclase</li>
            <li><strong>Etiquetas</strong>: Enteros (índice de clase), ej: 0, 1, 2, 3</li>
            <li>Es más eficiente en memoria que categorical crossentropy</li>
        </ul>

<pre><code class="language-python"># Etiquetas como enteros
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)</code></pre>

       <h3>Resumen</h3>

        <table>
            <thead>
                <tr>
                    <th>Función</th>
                    <th>Descripción</th>
                    <th>Capa de salida requerida</th>
                    <th>Formato de etiquetas</th>
                    <th>Ejemplo</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>binary_crossentropy</strong></td>
                    <td>Entropía cruzada para dos clases; mide discrepancia entre probabilidad predicha y etiqueta real</td>
                    <td>1 neurona + sigmoid</td>
                    <td>Valores 0 o 1</td>
                    <td>Spam/no spam, enfermo/sano</td>
                </tr>
                <tr>
                    <td><strong>categorical_crossentropy</strong></td>
                    <td>Entropía cruzada para múltiples clases mutuamente excluyentes</td>
                    <td>N neuronas + softmax</td>
                    <td>One-hot encoding: [0,1,0,0]</td>
                    <td>Clasificación de imágenes con etiquetas codificadas</td>
                </tr>
                <tr>
                    <td><strong>sparse_categorical_crossentropy</strong></td>
                    <td>Igual que categorical pero acepta etiquetas como enteros (más eficiente en memoria)</td>
                    <td>N neuronas + softmax</td>
                    <td>Enteros: 0, 1, 2, 3...</td>
                    <td>Clasificación de imágenes con etiquetas numéricas</td>
                </tr>
            </tbody>
        </table>

        <h2>3.2 Funciones para Regresión</h2>

        <h3>Mean Squared Error (MSE)</h3>
        <p class="formula">L = 1/N ∑<sub>i=1</sub><sup>N</sup>(y<sub>i</sub> - ŷ<sub>i</sub>)<sup>2</sup></p>
        <p>Penaliza fuertemente errores grandes. Es sensible a outliers.</p>

        <h3>Mean Absolute Error (MAE)</h3>
        <p class="formula">L = 1/N ∑<sub>i=1</sub><sup>N</sup>|y<sub>i</sub> - ŷ<sub>i</sub>|</p>
        <p>Más robusta ante outliers que MSE.</p>

        <h3>Huber Loss</h3>
        <p>Combina MSE y MAE: se comporta como MSE para errores pequeños y como MAE para errores grandes. Útil cuando hay outliers pero se quiere mantener sensibilidad a errores pequeños.</p>

<pre><code class="language-python"># Ejemplos de funciones de pérdida para regresión
model.compile(optimizer='adam', loss='mse')
model.compile(optimizer='adam', loss='mae')
model.compile(optimizer='adam', loss=keras.losses.Huber(delta=1.0))</code></pre>


        <h3>Resumen</h3>

        <table>
            <thead>
                <tr>
                    <th>Función</th>
                    <th>Descripción</th>
                    <th>Característica principal</th>
                    <th>Cuándo usarla</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>mse</strong> (Mean Squared Error)</td>
                    <td>Promedio del cuadrado de los errores</td>
                    <td>Penaliza mucho los errores grandes</td>
                    <td>Cuando errores grandes son muy costosos; sensible a outliers</td>
                </tr>
                <tr>
                    <td><strong>mae</strong> (Mean Absolute Error)</td>
                    <td>Promedio del valor absoluto de los errores</td>
                    <td>Trata todos los errores proporcionalmente</td>
                    <td>Cuando hay outliers; más robusta que MSE</td>
                </tr>
                <tr>
                    <td><strong>huber</strong></td>
                    <td>Combina MSE (errores pequeños) y MAE (errores grandes)</td>
                    <td>Balance entre sensibilidad y robustez</td>
                    <td>Cuando quieres sensibilidad a errores pequeños pero robustez ante outliers</td>
                </tr>
                <tr>
                    <td><strong>mape</strong> (Mean Absolute Percentage Error)</td>
                    <td>Error porcentual promedio</td>
                    <td>Independiente de la escala</td>
                    <td>Cuando importa el error relativo, no el absoluto</td>
                </tr>
                <tr>
                    <td><strong>msle</strong> (Mean Squared Logarithmic Error)</td>
                    <td>MSE aplicado a los logaritmos de las predicciones</td>
                    <td>Penaliza más subestimaciones que sobreestimaciones</td>
                    <td>Predicción de cantidades donde subestimar es peor (demanda, precios)</td>
                </tr>
            </tbody>
        </table>

    </section>


    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="script.js"></script>
</body>
</html>