<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ejercicio Práctico: Primeros Pasos con LM Studio</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div>Ejercicio Práctico: LM Studio</div>
    </header>

    <section class="contenido-didactico">
        <h1>Ejercicio Práctico: Primeros Pasos con LM Studio</h1>

        <h2>¿Qué es LM Studio?</h2>
        <p>
            LM Studio es una aplicación de escritorio gratuita que te permite <strong>ejecutar modelos de lenguaje grandes (LLMs) localmente</strong> en tu computadora, sin necesidad de internet ni APIs de pago. Soporta modelos en formato GGUF de Hugging Face.
        </p>

        <h2>Objetivo del Ejercicio</h2>
        <p>
            Crear un <strong>asistente de análisis de datos</strong> que te ayude a entender conceptos de IA y a generar código Python para análisis básicos.
        </p>

        <h2>Parte 1: Instalación y Configuración (15 min)</h2>

        <h3>1. Descargar LM Studio</h3>
        <ul>
            <li>Ve a: <a href="https://lmstudio.ai/" target="_blank">https://lmstudio.ai/</a></li>
            <li>Descarga la versión para tu sistema operativo (Windows/Mac/Linux)</li>
            <li>Instala la aplicación</li>
        </ul>

        <h3>2. Descargar un Modelo</h3>
        <p>
            Desde la pestaña <strong>"Discover"</strong> en LM Studio, busca y descarga <strong>UNO</strong> de estos modelos (ordenados de menor a mayor tamaño):
        </p>

        <p><strong>Opción ligera (4-8 GB RAM):</strong></p>
        <ul>
            <li><code>Phi-3-mini-4k-instruct-Q4_K_M.gguf</code> (~2.3 GB)</li>
        </ul>

        <p><strong>Opción recomendada (16 GB RAM):</strong></p>
        <ul>
            <li><code>Meta-Llama-3.2-3B-Instruct-Q4_K_M.gguf</code> (~2 GB)</li>
            <li><code>Mistral-7B-Instruct-v0.2-Q4_K_M.gguf</code> (~4 GB)</li>
        </ul>

        <p><strong>Opción potente (32 GB RAM o más):</strong></p>
        <ul>
            <li><code>Llama-3.1-8B-Instruct-Q5_K_M.gguf</code> (~5.7 GB)</li>
        </ul>

        <blockquote>
            <p><strong>Tip</strong>: Los modelos con <code>Q4_K_M</code> están cuantizados (comprimidos) para usar menos RAM manteniendo buena calidad.</p>
        </blockquote>

        <h3>3. Cargar el Modelo</h3>
        <ul>
            <li>Ve a la pestaña <strong>"Chat"</strong></li>
            <li>Haz clic en <strong>"Select a model to load"</strong></li>
            <li>Elige el modelo descargado</li>
            <li>Espera a que se cargue (verás la barra de progreso)</li>
        </ul>

        <h2>Parte 2: Ejercicio Práctico - Asistente de Análisis de Sentimientos</h2>

        <h3>Escenario</h3>
        <p>
            Tienes 5 reseñas de clientes y necesitas clasificarlas por sentimiento, pero quieres entender cómo hacerlo con Python antes de usar APIs de pago.
        </p>

        <h3>Instrucciones</h3>

        <p><strong>1. Copia estas reseñas de ejemplo:</strong></p>

        <pre><code class="language-plaintext">1. "El producto llegó rápido pero la calidad es horrible, no lo recomiendo"
2. "¡Excelente compra! Superó mis expectativas, volveré a comprar"
3. "Es un producto normal, nada especial pero cumple su función"
4. "Pésimo servicio al cliente, nunca respondieron mis mensajes"
5. "Me encanta, la mejor inversión que he hecho este año"</code></pre>

        <p><strong>2. Haz estas 3 consultas al modelo:</strong></p>

        <h4>Consulta 1: Análisis Manual</h4>
        <pre><code class="language-plaintext">Analiza el sentimiento de estas 5 reseñas y clasifícalas como:
- POSITIVO
- NEGATIVO  
- NEUTRAL

Reseñas:
[pega las 5 reseñas aquí]

Presenta los resultados en una tabla.</code></pre>

        <h4>Consulta 2: Generación de Código</h4>
        <pre><code class="language-plaintext">Escribe un código Python que:
1. Analice el sentimiento de una lista de textos
2. Use una biblioteca simple (sin ML complejas)
3. Clasifique cada texto como positivo, negativo o neutral
4. Cuente cuántas reseñas hay de cada tipo

Incluye comentarios explicativos en español.</code></pre>

        <h4>Consulta 3: Mejora del Código</h4>
        <pre><code class="language-plaintext">Mejora el código anterior para que:
1. Detecte también la intensidad del sentimiento (débil, moderado, fuerte)
2. Identifique palabras clave que influyeron en la clasificación
3. Genere un reporte en formato texto

Usa el siguiente texto de prueba:
"El producto es mediocre y el envío tardó mucho, aunque el empaque era bonito"</code></pre>

        <h2>Parte 3: Experimentación</h2>

        <p>Prueba a hacer estas preguntas adicionales:</p>

        <p><strong>Pregunta técnica:</strong></p>
        <pre><code class="language-plaintext">¿Cuál es la diferencia entre usar una API de NLP en la nube 
(como Azure Text Analytics) versus procesar el texto localmente 
con bibliotecas Python? Dame ventajas y desventajas de cada enfoque.</code></pre>

        <p><strong>Pregunta práctica:</strong></p>
        <pre><code class="language-plaintext">Si tengo un dataset con 10,000 reseñas de clientes, ¿qué enfoque 
me recomiendas y por qué?:
1. Usar APIs cloud (Google, Azure, AWS)
2. Entrenar mi propio modelo con AutoML
3. Usar bibliotecas open-source como TextBlob o VADER
4. Combinar varios enfoques

Considera factores de costo, precisión y mantenimiento.</code></pre>

        <h2>Bonus (Opcional)</h2>

        <h3>Experimenta con el Server Local</h3>

        <p>LM Studio puede crear una API local compatible con OpenAI:</p>

        <ol>
            <li>Ve a la pestaña <strong>"Local Server"</strong></li>
            <li>Selecciona tu modelo cargado</li>
            <li>Haz clic en <strong>"Start Server"</strong></li>
            <li>Usa este código Python para conectarte:</li>
        </ol>

        <pre><code class="language-python">from openai import OpenAI

# Conecta al servidor local de LM Studio
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

response = client.chat.completions.create(
    model="local-model",
    messages=[
        {"role": "system", "content": "Eres un experto en análisis de sentimientos."},
        {"role": "user", "content": "Analiza: 'Este producto es increíble'"}
    ],
    temperature=0.7,
)

print(response.choices[0].message.content)</code></pre>

        <h2>Configuración Recomendada para el Chat</h2>

        <p>En LM Studio, ajusta estos parámetros para mejores resultados:</p>

        <ul>
            <li><strong>Temperature</strong>: 0.3-0.5 (respuestas más consistentes para código)</li>
            <li><strong>Max Tokens</strong>: 2000-4000</li>
            <li><strong>Context Length</strong>: 4096 o mayor</li>
            <li><strong>System Prompt</strong>: "Eres un profesor de IA que explica conceptos técnicos de forma clara y genera código Python bien comentado."</li>
        </ul>

        <h2>Preguntas de Reflexión Final</h2>

        <ol>
            <li><strong>Velocidad</strong>: ¿Cuánto tardó el modelo en responder comparado con ChatGPT/Claude?</li>
            <li><strong>Calidad</strong>: ¿Las respuestas fueron coherentes y útiles?</li>
            <li><strong>Privacidad</strong>: ¿Qué ventaja tiene procesar datos sensibles localmente?</li>
            <li><strong>Limitaciones</strong>: ¿Qué limitaciones notaste vs. modelos en la nube?</li>
            <li><strong>Casos de uso</strong>: ¿Cuándo usarías LM Studio en un proyecto profesional?</li>
        </ol>

        <h2>Recursos Adicionales</h2>

        <ul>
            <li><strong>Documentación</strong>: <a href="https://lmstudio.ai/docs" target="_blank">https://lmstudio.ai/docs</a></li>
            <li><strong>Modelos recomendados</strong>: <a href="https://huggingface.co/models?library=gguf" target="_blank">https://huggingface.co/models?library=gguf</a></li>
            <li><strong>Comunidad</strong>: Discord de LM Studio para soporte</li>
        </ul>


    </section>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="script.js"></script>
</body>
</html>